{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [A Random Forest analysis in Python](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/improving-your-predictions-through-random-forests?ex=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A detailed study of Random Forests would take this tutorial a bit too far. However, since it's an often used machine learning technique, gaining a general understanding in Python won't hurt.\n",
    "\n",
    "In layman's terms, the Random Forest technique handles the overfitting problem you faced with decision tree. It grows multiple (very deep) classification trees using the training set. At the time of prediction, each tree is used to come up with a prediction and every outcome is counted as a vote. For example, if you have trained 3 trees with 2 saying a passenger in the test set will survive and 1 days he will not, the passenger will be classified as a survivor. This approach of overtraining trees, but having the majority's vote count as the actual classification decision, avoids overfitting.\n",
    "\n",
    "Building a random forest in Python looks almost the same as building a decision tree; so we can jump right to it. There are two key differences, however. Firstly, a different class is used. And second, a new argument is necessary. Also, we need to import the necessary library from scikit-learn.\n",
    "* Use `RandomForestClassifier()` class instead of the `DecisionTreeClassifier()` class\n",
    "* `n_estimators` needs to be set when using the `RandomForestClassifier()` class. This argument allows you to set the number of trees you wish to plant and average over.\n",
    "\n",
    "The latest training and testing data are preloaded for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Build the random forest with `n_estimators` set to `100`.\n",
    "* Fit your random forest model with inputs `features_forest` and `target`.\n",
    "* Compute the classifier predictions on the selected test set features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939393939394\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train and test datasets to create two DataFrames\n",
    "train_url = 'http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv'\n",
    "train = pd.read_csv(train_url)\n",
    "\n",
    "test_url = 'http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv'\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "train.Age = train.Age.fillna(train.Age.median())\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "train.loc[train.Sex == 'male', 'Sex'] = 0\n",
    "train.loc[train.Sex == 'female', 'Sex'] = 1\n",
    "test.loc[test.Sex == 'male', 'Sex'] = 0\n",
    "test.loc[test.Sex == 'female', 'Sex'] = 1\n",
    "\n",
    "# Impute the Embarked variable\n",
    "train.Embarked = train.Embarked.fillna('S')\n",
    "test.Embarked = test.Embarked.fillna('S')\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "train.loc[train.Embarked == 'S', 'Embarked'] = 0\n",
    "train.loc[train.Embarked == 'C', 'Embarked'] = 1\n",
    "train.loc[train.Embarked == 'Q', 'Embarked'] = 2\n",
    "test.loc[test.Embarked == 'S', 'Embarked'] = 0\n",
    "test.loc[test.Embarked == 'C', 'Embarked'] = 1\n",
    "test.loc[test.Embarked == 'Q', 'Embarked'] = 2\n",
    "\n",
    "target = train.Survived.values\n",
    "\n",
    "\n",
    "test.Age = test.Age.fillna(test.Age.median())\n",
    "\n",
    "# Impute the missing value with the median\n",
    "test.loc[152, 'Fare'] = test.Fare.median()\n",
    "\n",
    "# Import the `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# We wan the Pclass, Age, Sex, Fare, SibSp, Parch, and Embarked variables\n",
    "features_forest = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Building and fitting my_forest\n",
    "forest = RandomForestClassifier(max_depth = 10, min_samples_split = 2, n_estimators = 100, random_state = 1)\n",
    "my_forest = forest.fit(features_forest, target)\n",
    "\n",
    "# Print the score of the fitted random forest\n",
    "print(my_forest.score(features_forest, target))\n",
    "\n",
    "# Compute predictions on our test set features then prin the length of prediction vector\n",
    "test_features = test[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "pred_forest = my_forest.predict(test_features)\n",
    "print(len(pred_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Interpreting and Comparing](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/improving-your-predictions-through-random-forests?ex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Remember how we looked at `.feature_importances_` attribute for the decision trees? Well, you can request the same attribution from your random forest as well and interpret the relevance of the included variables. You might also want to compare the models in some quick and easy way. For this, we can use the `.score()` method. The `.score()` method takes the features data and the target vector and computes mean accuracy of your model. You can apply this method to both the forest and individual trees. Remember, this measure should be high but not extreme because that would be a sign of overfitting.\n",
    "\n",
    "For this exercise, you have `my_forest` and `my_tree_two` available to you. The features and target arrays are also ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Explore the feature importance for both models\n",
    "* Compare the mean accuracy score of the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14130255  0.17906027  0.41616727  0.17938711  0.05039699  0.01923751\n",
      "  0.0144483 ]\n",
      "[ 0.10384741  0.20139027  0.31989322  0.24602858  0.05272693  0.04159232\n",
      "  0.03452128]\n",
      "0.905723905724\n",
      "0.939393939394\n"
     ]
    }
   ],
   "source": [
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn import tree\n",
    "\n",
    "# Create a new array with the added features: features_two\n",
    "features_two = train[['Pclass', 'Age', 'Sex', 'Fare', 'SibSp', 'Parch', 'Embarked']].values\n",
    "\n",
    "# Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5: my_tree_two\n",
    "max_depth = 10\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split, random_state = 1)\n",
    "my_tree_two = my_tree_two.fit(features_two, target)\n",
    "\n",
    "# Request and print the `.feature_importances_` attribute\n",
    "print(my_tree_two.feature_importances_)\n",
    "print(my_forest.feature_importances_)\n",
    "\n",
    "# Compute and print the mean accuracy score for both models\n",
    "print(my_tree_two.score(features_two, target))\n",
    "print(my_forest.score(features_forest, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Conclude and Submit](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/improving-your-predictions-through-random-forests?ex=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on your finding in the previous exercise determine which feature was of most importance, and for which model. After this final exercise, you will be able to submit your random forest model Kaggle! Use `my_forest`, `my_tree_two`, and `feature_importances_` to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The most important feature was \"Sex\", but it was more significant for \"my_tree_two\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wonderful! You are now at the end of this tutorial and ready to start improving the results yourself\n",
    "\n",
    "You have finished the chapter \"Improving your predictions through Random Forests\"!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
