{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391b4d41-2ef8-4a06-be86-a06612482b3f",
   "metadata": {},
   "source": [
    "When you interact with an LLM, naturally, it doesn't remember the previous messages.\n",
    "We can overcome this by creating `memory`.\n",
    "`LangChain` offers multiple types of `memory` management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83005a5-33b5-40f6-b608-97ffeb23b0d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9746e335-6306-44ab-813f-15100d09a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c10eb-7eab-47ed-8294-e8835dbe57c8",
   "metadata": {},
   "source": [
    "We initialize our `model` as normal, and initialize a `ConversationChain` with the model and add `memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432bbc83-2810-4dca-b594-0353b74a95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = \"2023-12-01-preview\"\n",
    "deployment_id = \"gpt-35-turbo-16k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e80fd42-0244-49ea-b034-c1b49d5aa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(model=deployment_id, temperature=0.0, api_version=api_version)\n",
    "memory = ConversationBufferMemory()\n",
    "convo = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e8213-7207-49a2-9d48-c9f9481d2f59",
   "metadata": {},
   "source": [
    "Note that we use the `predict` method with keyword argument `input`.\n",
    "This is a result of using a `chain` instead of the `chat` as in the first lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9830c1f3-0992-430d-8fee-321dacf616ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Ian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"Hi, my name is Ian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cf6165-ba0d-4c02-ae0c-c7487a758672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae37a48-87f4-4a41-b31e-4e82a5b85182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Ian.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba07c87-fc3a-4f2b-bbe8-954afebb4887",
   "metadata": {},
   "source": [
    "Because we are using `memory`, the model can remember our prior messages.\n",
    "If we set `verbose=True` in the `ConversationChain` we can see more of what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961c3c0d-882e-4e82-ba80-be394c7e9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "convo = ConversationChain(\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9904ed1-ba0e-42cd-982b-816cbd71aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Ian\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Ian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"Hi, my name is Ian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bf07e-7b2f-4ac3-9731-d671a968f535",
   "metadata": {},
   "source": [
    "Everything that is in <span style=\"color:green\">green</span> is a part of the memory and any internal prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b1c4fe-befc-4f46-9696-c03624073a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08b890-5d46-42b2-822a-49de25d89dd3",
   "metadata": {},
   "source": [
    "As the conversation grows, you can see the <span style=\"color:green\">Current conversation</span> being updated with prior messages.\n",
    "The prompt tells the LLM what has already been said, giving it context to answer follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e67f7a-56ad-4eed-bb44-c13038e7300b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Ian.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2675a-980d-41d9-a8e9-34e234df24f7",
   "metadata": {},
   "source": [
    "Hence why it knows what my name is ðŸ™‚."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd909a5-029d-4fa3-b4ba-1c98a4e5e9ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Side Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d52d10-df67-4135-bfd1-f3ece5452cb4",
   "metadata": {},
   "source": [
    "Here is what it looks like if we exclude `memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2316b154-c035-41db-824a-b023e1d24308",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(model=deployment_id, temperature=0.0, api_version=api_version)\n",
    "convo = ConversationChain(\n",
    "    verbose=True,\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a87c7d-87c7-482d-a380-0ebdf13a979c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Ian\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Ian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"Hi, my name is Ian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8569baa-a427-4398-bb16-e17287e860d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6c37cf-9c9d-4315-9925-368433dec514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Ian.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc235687-99e8-4bb1-b8e1-1764e71931a7",
   "metadata": {},
   "source": [
    "It actually looks like the `memory` argument didn't need to be set ðŸ¤”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d85b54fa-a88e-4480-b42a-35ad90be7555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mConversationChain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmemory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtags\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasePromptTemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'history'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mllm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompt_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPromptValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompt_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPromptValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'response'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_parser\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_parsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseLLMOutputParser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreturn_final_only\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mllm_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minput_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Chain to have a conversation and load context from memory.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain.chains import ConversationChain\n",
       "        from langchain_community.llms import OpenAI\n",
       "\n",
       "        conversation = ConversationChain(llm=OpenAI())\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\a2644752\\sandbox\\private-rai\\env\\lib\\site-packages\\langchain\\chains\\conversation\\base.py\n",
       "\u001b[1;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b5901-cabd-4425-9fa1-d0babd63f4e2",
   "metadata": {},
   "source": [
    "The docs say that the `memory` argument should subclass from `langchain_core.memory.BaseMemory` but defaults to `None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89e3a1-f853-4c2a-9c09-f7ed5c4d6d67",
   "metadata": {},
   "source": [
    "In\n",
    "```python\n",
    "from inspect import getsource\n",
    "\n",
    "print(getsource(ConversationChain))\n",
    "```\n",
    "\n",
    "Out\n",
    "```python\n",
    "class ConversationChain(LLMChain):\r\n",
    "    \"\"\"Chain to have a conversation and load context from memory.\r\n",
    "\r\n",
    "    Example:\r\n",
    "        .. code-block:: python\r\n",
    "\r\n",
    "            from langchain.chains import ConversationChain\r\n",
    "            from langchain_community.llms import OpenAI\r\n",
    "\r\n",
    "            conversation = ConversationChain(llm=OpenAI())\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    memory: BaseMemory = Field(default_factory=ConversationBufferMemory)\r\n",
    "    \"\"\"Default memory store.\"\"\"\r\n",
    "    prompt: BasePromptTemplate = PROMPT\r\n",
    "    \"\"\"Default conversation prompt to use.\"\"\"\r\n",
    "\r\n",
    "    input_key: str = \"input\"  #: :meta private:\r\n",
    "    output_key: str = \"response\"  #: :meta private:\r\n",
    "\r\n",
    "    class Config:\r\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\r\n",
    "\r\n",
    "        extra = Extra.forbid\r\n",
    "        arbitrary_types_allowed = True\r\n",
    "\r\n",
    "    @classmethod\r\n",
    "    def is_lc_serializable(cls) -> bool:\r\n",
    "        return False\r\n",
    "\r\n",
    "    @property\r\n",
    "    def input_keys(self) -> List[str]:\r\n",
    "        \"\"\"Use this since so some prompt vars come from history.\"\"\"\r\n",
    "        return [self.input_key]\r\n",
    "\r\n",
    "    @root_validator()\r\n",
    "    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\r\n",
    "        \"\"\"Validate that prompt input variables are consistent.\"\"\"\r\n",
    "        memory_keys = values[\"memory\"].memory_variables\r\n",
    "        input_key = values[\"input_key\"]\r\n",
    "        if input_key in memory_keys:\r\n",
    "            raise ValueError(\r\n",
    "                f\"The input key {input_key} was also found in the memory keys \"\r\n",
    "                f\"({memory_keys}) - please provide keys that don't overlap.\"\r\n",
    "            )\r\n",
    "        prompt_variables = values[\"prompt\"].input_variables\r\n",
    "        expected_keys = memory_keys + [input_key]\r\n",
    "        if set(expected_keys) != set(prompt_variables):\r\n",
    "            raise ValueError(\r\n",
    "                \"Got unexpected prompt input variables. The prompt expects \"\r\n",
    "                f\"{prompt_variables}, but got {memory_keys} as inputs from \"\r\n",
    "                f\"memory, and {input_key} as the normal input key.\"\r\n",
    "            )\r\n",
    "        return values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdf3e7-e962-45ea-aca8-f5141a9d70c3",
   "metadata": {},
   "source": [
    "Looking at the source code, `memory` is set to:\n",
    "\n",
    "```python\n",
    "memory: BaseMemory = Field(default_factory=ConversationBufferMemory)\n",
    "```\n",
    "\n",
    "This means that if we don't provide an argument for `memory`, a `ConversationBufferMemory` instance is used by default.\n",
    "Here is an example function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf62c624-d143-4907-bfd4-f6099d0e9c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from pydantic.fields import Field\n",
    "from pydantic.main import BaseModel\n",
    "\n",
    "class Example(BaseModel):\n",
    "    \"\"\"This is an example class.\n",
    "    \n",
    "    It highlights how the `default_factory` argument works in `Field`.\n",
    "    \"\"\"\n",
    "\n",
    "    counter: dict = Field(default_factory=Counter)\n",
    "\n",
    "\n",
    "eg = Example()\n",
    "print(eg.counter)  # Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97186ac3-ef18-4417-ba96-af6f0aa6e57a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Continuing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6d040-a7f3-4630-9474-d9ecfce28cf6",
   "metadata": {},
   "source": [
    "If we look at the `memory` instance we can view what has been added to its `buffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "141abd55-90a4-476e-bbe0-de88b392dbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Ian.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "convo = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "convo.predict(input=\"Hi, my name is Ian\")\n",
    "convo.predict(input=\"What is 1+1?\")\n",
    "convo.predict(input=\"What is my name?\")\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7245a9-bc8e-492f-80f9-1c424c6e6823",
   "metadata": {},
   "source": [
    "We can also view the memory as a dictionary of variables, with `history` holding the prior messages in string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69e1345d-29ff-4add-9367-83f479849c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Ian\\nAI: Hello Ian! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Ian.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aba4db-ea1e-477a-98e1-3c4080793a7a",
   "metadata": {},
   "source": [
    "The `memory` doesn't have to be modified by an LLM -- we can update it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1588df75-cf90-42c3-b511-d08c2ff72207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(inputs={\"input\": \"Hi\"}, outputs={\"output\": \"What's up\"})\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1639ed4d-e0ee-4cf6-a255-0ca4e1674597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d175bee-b5c7-4920-a2a2-8c26a49ccfd2",
   "metadata": {},
   "source": [
    "As we add to more inputs and outputs to the context, the buffer is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16191760-aa88-46dc-b1af-608e88b2434e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(inputs={\"input\": \"Not much, just hanging\"}, outputs={\"output\": \"Cool\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb68e0-d296-4b2b-a495-9f4a83ac5c0c",
   "metadata": {},
   "source": [
    "An LLM is stateless and doesn't remember the conversation by default.\n",
    "Chat history appears in the context provided to the LLM with the `memory` object.\n",
    "As the conversation becomes long, the cost increases.\n",
    "`LangChain` provides convenient `memory` types to handle this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6e794-cbf3-4466-8e39-764c9862fbdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9a02c-6c5b-4af4-8061-5fd2db26f9fd",
   "metadata": {},
   "source": [
    "The `ConversationBufferWindowMemory` stores context within a \"window\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f0628e0-133e-463c-9876-613e3f97ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "908e9fa2-ad56-4df7-8d30-8464735033b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cf4a3-e930-4491-81e8-e8bcf2871f2e",
   "metadata": {},
   "source": [
    "The variable `k` tells the `memory` how many message inputs to keep in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6fe8f0d-fdb5-4925-81ab-8715103a8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs={\"input\": \"Hi\"}, outputs={\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f8bdf4-3dd3-4030-b22f-0296ef2d3b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2eaca326-39f7-459e-b54e-0713b867daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs={\"input\": \"Not much, just hanging\"}, outputs={\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97943e42-6f23-47c1-8fa1-99109724f5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Not much, just hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7b311-fb39-4c21-9fda-3ff4f3dc348f",
   "metadata": {},
   "source": [
    "Notice that we lose the prior message inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f1bc11d-2459-485c-b655-09e01b411ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "convo = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc71e763-e158-453a-a8c5-c1580864c37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Ian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"Hi, my name is Ian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0034302-11b1-44d2-b09d-0ba1fa2b4c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e27199f8-8dd8-4301-989d-badf508940d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2c60a-b6b2-4b9e-96d9-f7086951a8bb",
   "metadata": {},
   "source": [
    "Because the memory drops off after each input with `k=1`, it makes sense that the LLM can't tell us my name.\n",
    "\n",
    "Running with `verbose=True` we can see the context provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "979a2152-bf18-4824-993c-8ff39f3ebab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "convo = ConversationChain(\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fcf09fa-87bf-4f54-9c47-f9e7a7740692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Ian\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Ian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"Hi, my name is Ian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "434e1383-db57-404d-b73a-b1fe0055d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be7f3e0a-1819-485f-8048-8b37792c1fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4275d-bc93-49fc-a5fa-610adc53e75c",
   "metadata": {},
   "source": [
    "# `ConversationTokenBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e050fa9-31e4-4dc5-98bb-f4a2973cbd4d",
   "metadata": {},
   "source": [
    "The `ConverstationTokenBufferMemory` operates similar to the `ConversationBufferWindowMemory`, but limits tokens instead of input messages.\n",
    "This is beneficial as we are charged by the token count rather than the message count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9c38054-9838-4eaf-aff1-26b1772ee4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tiktoken\n",
      "Version: 0.5.1\n",
      "Summary: tiktoken is a fast BPE tokeniser for use with OpenAI's models\n",
      "Home-page: \n",
      "Author: Shantanu Jain\n",
      "Author-email: shantanu@openai.com\n",
      "License: MIT License\n",
      "\n",
      "Copyright (c) 2022 OpenAI, Shantanu Jain\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "Location: C:\\Users\\A2644752\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\n",
      "Requires: regex, requests\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c236b91c-5b20-4ea5-ab1f-152810dfe130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c31b5f57-6333-4c4c-8f44-7a145ba2b6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.354'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81c6a68c-bfef-4a4f-b97a-da6a38073ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938b876-4eb1-428f-b3fa-ad6781826bc2",
   "metadata": {},
   "source": [
    "We have to specify the `llm` argument because it determines how tokens are counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a469526-e945-4b7c-979c-95c3f1539ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo-16k.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI is what?!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAmazing!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackpropagation is what?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeautiful!\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m      3\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbots are what?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharming!\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain\\memory\\token_buffer.py:54\u001b[0m, in \u001b[0;36mConversationTokenBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[0;32m     53\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m---> 54\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m     56\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:620\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    618\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    622\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/openai/openai-python/blob/main/chatml.md for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    625\u001b[0m     )\n\u001b[0;32m    626\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    627\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo-16k.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "memory.save_context(inputs={\"input\": \"AI is what?!\"}, outputs={\"output\": \"Amazing!\"})\n",
    "memory.save_context(inputs={\"input\": \"Backpropagation is what?\"}, outputs={\"output\": \"Beautiful!\"})\n",
    "memory.save_context(inputs={\"input\": \"Chatbots are what?\"}, outputs={\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba819397-9638-4447-91c8-272725434bab",
   "metadata": {},
   "source": [
    "The `ConversationTokenBufferMemory` doesn't currently work for `\"gpt-35-turbo-16k\"`, so I'm moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345da3b-abe9-42c9-b130-b54fc9e95748",
   "metadata": {},
   "source": [
    "# `ConversationSummaryBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa26cf-94ac-4e4f-af8c-37aecab0da17",
   "metadata": {},
   "source": [
    "Instead of determining a hard cutoff for the LLM context, we can summarize prior messages allowing us to have \"infinite\" memory, though with some loss the longer the conversation goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bb939e6-fa2d-46b7-8a55-9ff0312cebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9a243-f7e6-48c3-b18c-a1a24571c5b2",
   "metadata": {},
   "source": [
    "We highlight the power of summarization by using a \"long\" string as our initial context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da1be745-0a49-4df8-b7f9-f05489f2d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = \"\"\"There is a meeting at 8am with your product team.\n",
    "You will need your powerpoint presenation prepared.\n",
    "9am-12pm have time to work on your LangChain\n",
    "project which will go quickly because LangChain is such a powerful tool\n",
    "At Noon, lunch at the italian restaurant with a customer who is driving\n",
    "from over an hour away to meet you to understand the latest in AI.\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adedea32-1959-4995-9c68-de575b288e86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo-16k.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationSummaryBufferMemory(llm\u001b[38;5;241m=\u001b[39mchat, max_token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms up\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot much, just hanging\u001b[39m\u001b[38;5;124m\"\u001b[39m}, outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCool\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m      5\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is on the schedule today?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschedule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:60\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:65\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m---> 65\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m     67\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:620\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    618\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    622\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/openai/openai-python/blob/main/chatml.md for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    625\u001b[0m     )\n\u001b[0;32m    626\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    627\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo-16k.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=100)\n",
    "\n",
    "memory.save_context(inputs={\"input\": \"Hello\"}, outputs={\"output\": \"What's up\"})\n",
    "memory.save_context(inputs={\"input\": \"Not much, just hanging\"}, outputs={\"output\": \"Cool\"})\n",
    "memory.save_context(inputs={\"input\": \"What is on the schedule today?\"}, outputs={\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c028691-e45c-46a8-82dc-7803c2d048c4",
   "metadata": {},
   "source": [
    "Apparently `ConversationSummaryBufferMemory` doesn't work with `\"gpt-35-turbo-16k\"` either ðŸ˜‘."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
