{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391b4d41-2ef8-4a06-be86-a06612482b3f",
   "metadata": {},
   "source": [
    "When you interact with an LLM, naturally, it doesn't remember the previous messages.\n",
    "We can overcome this by creating `memory`.\n",
    "`LangChain` offers multiple types of `memory` management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83005a5-33b5-40f6-b608-97ffeb23b0d6",
   "metadata": {},
   "source": [
    "# `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9746e335-6306-44ab-813f-15100d09a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c10eb-7eab-47ed-8294-e8835dbe57c8",
   "metadata": {},
   "source": [
    "We initialize our `model` as normal, and initialize a `ConversationChain` with the model and add `memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432bbc83-2810-4dca-b594-0353b74a95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = \"2023-12-01-preview\"\n",
    "deployment_id = \"gpt-35-turbo-16k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e80fd42-0244-49ea-b034-c1b49d5aa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(model=deployment_id, temperature=0.0, api_version=api_version)\n",
    "memory = ConversationBufferMemory()\n",
    "convo = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e8213-7207-49a2-9d48-c9f9481d2f59",
   "metadata": {},
   "source": [
    "Note that we use the `predict` method with keyword argument `input`.\n",
    "This is a result of using a `chain` instead of the `chat` as in the first lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9830c1f3-0992-430d-8fee-321dacf616ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Ian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"Hi, my name is Ian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cf6165-ba0d-4c02-ae0c-c7487a758672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae37a48-87f4-4a41-b31e-4e82a5b85182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Ian.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba07c87-fc3a-4f2b-bbe8-954afebb4887",
   "metadata": {},
   "source": [
    "Because we are using `memory`, the model can remember our prior messages.\n",
    "If we set `verbose=True` in the `ConversationChain` we can see more of what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961c3c0d-882e-4e82-ba80-be394c7e9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "convo = ConversationChain(\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9904ed1-ba0e-42cd-982b-816cbd71aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Ian\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Ian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"Hi, my name is Ian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bf07e-7b2f-4ac3-9731-d671a968f535",
   "metadata": {},
   "source": [
    "Everything that is in <span style=\"color:green\">green</span> is a part of the memory and any internal prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b1c4fe-befc-4f46-9696-c03624073a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08b890-5d46-42b2-822a-49de25d89dd3",
   "metadata": {},
   "source": [
    "As the conversation grows, you can see the <span style=\"color:green\">Current conversation</span> being updated with prior messages.\n",
    "The prompt tells the LLM what has already been said, giving it context to answer follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e67f7a-56ad-4eed-bb44-c13038e7300b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Ian.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2675a-980d-41d9-a8e9-34e234df24f7",
   "metadata": {},
   "source": [
    "Hence why it knows what my name is ðŸ™‚."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd909a5-029d-4fa3-b4ba-1c98a4e5e9ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Side Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d52d10-df67-4135-bfd1-f3ece5452cb4",
   "metadata": {},
   "source": [
    "Here is what it looks like if we exclude `memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2316b154-c035-41db-824a-b023e1d24308",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(model=deployment_id, temperature=0.0, api_version=api_version)\n",
    "convo = ConversationChain(\n",
    "    verbose=True,\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a87c7d-87c7-482d-a380-0ebdf13a979c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Ian\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Ian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"Hi, my name is Ian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8569baa-a427-4398-bb16-e17287e860d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6c37cf-9c9d-4315-9925-368433dec514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Ian.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc235687-99e8-4bb1-b8e1-1764e71931a7",
   "metadata": {},
   "source": [
    "It actually looks like the `memory` argument didn't need to be set ðŸ¤”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d85b54fa-a88e-4480-b42a-35ad90be7555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mConversationChain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmemory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtags\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasePromptTemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'history'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mllm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompt_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPromptValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompt_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPromptValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'response'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_parser\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_parsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseLLMOutputParser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreturn_final_only\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mllm_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minput_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Chain to have a conversation and load context from memory.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain.chains import ConversationChain\n",
       "        from langchain.llms import OpenAI\n",
       "\n",
       "        conversation = ConversationChain(llm=OpenAI())\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\a2644752\\sandbox\\private-rai\\env\\lib\\site-packages\\langchain\\chains\\conversation\\base.py\n",
       "\u001b[1;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b5901-cabd-4425-9fa1-d0babd63f4e2",
   "metadata": {},
   "source": [
    "The docs say that the `memory` argument should subclass from `langchain_core.memory.BaseMemory` but defaults to `None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37709043-334c-4879-afb3-b5741a951ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getsource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ea1118-dcfb-4779-b881-695abe7cd49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ConversationChain(LLMChain):\n",
      "    \"\"\"Chain to have a conversation and load context from memory.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.chains import ConversationChain\n",
      "            from langchain.llms import OpenAI\n",
      "\n",
      "            conversation = ConversationChain(llm=OpenAI())\n",
      "    \"\"\"\n",
      "\n",
      "    memory: BaseMemory = Field(default_factory=ConversationBufferMemory)\n",
      "    \"\"\"Default memory store.\"\"\"\n",
      "    prompt: BasePromptTemplate = PROMPT\n",
      "    \"\"\"Default conversation prompt to use.\"\"\"\n",
      "\n",
      "    input_key: str = \"input\"  #: :meta private:\n",
      "    output_key: str = \"response\"  #: :meta private:\n",
      "\n",
      "    class Config:\n",
      "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
      "\n",
      "        extra = Extra.forbid\n",
      "        arbitrary_types_allowed = True\n",
      "\n",
      "    @classmethod\n",
      "    def is_lc_serializable(cls) -> bool:\n",
      "        return False\n",
      "\n",
      "    @property\n",
      "    def input_keys(self) -> List[str]:\n",
      "        \"\"\"Use this since so some prompt vars come from history.\"\"\"\n",
      "        return [self.input_key]\n",
      "\n",
      "    @root_validator()\n",
      "    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\n",
      "        \"\"\"Validate that prompt input variables are consistent.\"\"\"\n",
      "        memory_keys = values[\"memory\"].memory_variables\n",
      "        input_key = values[\"input_key\"]\n",
      "        if input_key in memory_keys:\n",
      "            raise ValueError(\n",
      "                f\"The input key {input_key} was also found in the memory keys \"\n",
      "                f\"({memory_keys}) - please provide keys that don't overlap.\"\n",
      "            )\n",
      "        prompt_variables = values[\"prompt\"].input_variables\n",
      "        expected_keys = memory_keys + [input_key]\n",
      "        if set(expected_keys) != set(prompt_variables):\n",
      "            raise ValueError(\n",
      "                \"Got unexpected prompt input variables. The prompt expects \"\n",
      "                f\"{prompt_variables}, but got {memory_keys} as inputs from \"\n",
      "                f\"memory, and {input_key} as the normal input key.\"\n",
      "            )\n",
      "        return values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getsource(ConversationChain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdf3e7-e962-45ea-aca8-f5141a9d70c3",
   "metadata": {},
   "source": [
    "Looking at the source code, `memory` is set to:\n",
    "\n",
    "```python\n",
    "memory: BaseMemory = Field(default_factory=ConversationBufferMemory)\n",
    "```\n",
    "\n",
    "This means that if we don't provide an argument for `memory`, a `ConversationBufferMemory` instance is used by default.\n",
    "Here is an example function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf62c624-d143-4907-bfd4-f6099d0e9c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from pydantic.fields import Field\n",
    "from pydantic.main import BaseModel\n",
    "\n",
    "class Example(BaseModel):\n",
    "    \"\"\"This is an example class.\n",
    "    \n",
    "    It highlights how the `default_factory` argument works in `Field`.\n",
    "    \"\"\"\n",
    "\n",
    "    counter: dict = Field(default_factory=Counter)\n",
    "\n",
    "\n",
    "eg = Example()\n",
    "print(eg.counter)  # Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97186ac3-ef18-4417-ba96-af6f0aa6e57a",
   "metadata": {},
   "source": [
    "## Continuing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6d040-a7f3-4630-9474-d9ecfce28cf6",
   "metadata": {},
   "source": [
    "If we look at the `memory` instance we can view what has been added to its `buffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "141abd55-90a4-476e-bbe0-de88b392dbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Ian\n",
      "AI: Hello Ian! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Ian.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "convo = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "convo.predict(input=\"Hi, my name is Ian\")\n",
    "convo.predict(input=\"What is 1+1?\")\n",
    "convo.predict(input=\"What is my name?\")\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7245a9-bc8e-492f-80f9-1c424c6e6823",
   "metadata": {},
   "source": [
    "We can also view the memory as a dictionary of variables, with `history` holding the prior messages in string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69e1345d-29ff-4add-9367-83f479849c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Ian\\nAI: Hello Ian! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Ian.\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aba4db-ea1e-477a-98e1-3c4080793a7a",
   "metadata": {},
   "source": [
    "The `memory` doesn't have to be modified by an LLM -- we can update it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1588df75-cf90-42c3-b511-d08c2ff72207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(inputs={\"input\": \"Hi\"}, outputs={\"output\": \"What's up\"})\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1639ed4d-e0ee-4cf6-a255-0ca4e1674597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d175bee-b5c7-4920-a2a2-8c26a49ccfd2",
   "metadata": {},
   "source": [
    "As we add to more inputs and outputs to the context, the buffer is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16191760-aa88-46dc-b1af-608e88b2434e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(inputs={\"input\": \"Not much, just hanging\"}, outputs={\"output\": \"Cool\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f9d369-7f04-48c0-8119-9f62c9ef9c01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3c6e794-cbf3-4466-8e39-764c9862fbdb",
   "metadata": {},
   "source": [
    "# `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30716981-8b77-475d-afaa-e30688b01972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
