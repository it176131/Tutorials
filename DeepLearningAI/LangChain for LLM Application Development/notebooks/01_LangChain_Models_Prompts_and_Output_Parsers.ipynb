{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98028637-e9c7-4248-9191-0f8fe64d2185",
   "metadata": {},
   "source": [
    "\"Models\" refer to (Large) Language Models.\n",
    "\"Prompts\" refer to the style of creating inputs to pass to the models.\n",
    "\"Parsers\" refer to taking the output of the models and parsing it into a more structured format that can be used downstream.\n",
    "\n",
    "When you build an application using an LLM, the model will be reusable.\n",
    "We repeatedly prompt a model and parse the output.\n",
    "`LangChain` gives an easy set of abstractions to do this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59b3f23-26f9-4891-b605-136f526af666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec529c2-7c41-40fa-a90c-1ae544610ce1",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0676a9-d3a7-47a1-92e0-96a04b73d62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.2.3\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: C:\\Users\\A2644752\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, tqdm, typing-extensions\n",
      "Required-by: instructor\n"
     ]
    }
   ],
   "source": [
    "!pip show openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ccdc26-939c-44a8-8d06-21a5f999e34b",
   "metadata": {},
   "source": [
    "OpenAI version 1.0.0 introduced an API change.\n",
    "See [here](https://github.com/openai/openai-python/discussions/742) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3af4719-a58e-461a-9d8d-a9fd03440280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai.lib.azure import AzureOpenAI\n",
    "from openai.types.chat.chat_completion import ChatCompletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653cadd-10de-43ed-bfed-8ccbff72f78b",
   "metadata": {},
   "source": [
    "Available API versions can be found [here](https://github.com/Azure/azure-rest-api-specs/tree/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference).\n",
    "- [Preview](https://github.com/Azure/azure-rest-api-specs/tree/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview)\n",
    "- [Stable](https://github.com/Azure/azure-rest-api-specs/tree/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb894051-9c2a-49c8-9fb0-129fa3dac102",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = \"2023-12-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d244002-20c5-4dd9-81c7-010332ba8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = AzureOpenAI(api_version=api_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9348a8-8ed9-480d-9493-94fd1771d9b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "My Azure account requires the deployment name rather than the model name.\n",
    "Use `model: str = \"gpt-35-turbo-16k\"` instead of `model: str = \"gpt-3.5-turbo\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b02f2-5848-4e4e-b7a3-c9415c1c888c",
   "metadata": {},
   "source": [
    "`openai.ChatCompletion.create` was replaced with:\n",
    "- `AzureOpenAI(...).chat.completions.create` if using AzureOpenAI\n",
    "- `OpenAI(...).chat.completions.create` if using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f2526a-5198-415f-aa88-abb2d007ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt: str, model: str = \"gpt-35-turbo-16k\") -> str:\n",
    "    \"\"\"Given a prompt and model, return the content of the response.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response: ChatCompletion = openai.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1908119-8435-4ffc-b673-083e978209b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309663d2-9801-4ae8-8f2f-8b31dc73f5e0",
   "metadata": {},
   "source": [
    "To motivate the LangChain abstractions for model prompts and parsers, suppose you get an email in a language other than english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7a9a431-b6c5-45e2-a556-cd0a4c5b32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Arr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6262d4-7e33-41d8-a60c-258a81d95a56",
   "metadata": {},
   "source": [
    "We can translate the message by asking the LLM to rewrite it in a given style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d929e79c-0bcf-49c6-b9a4-051bd32d61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af6edda9-b58d-4909-aaa9-d33bbf9d133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: ```{email}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88959506-44e8-44a1-8aba-0c3456581e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks\n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a827fa01-cd1e-4bae-be8a-852a8d904deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "438dcc0c-9de9-41bc-a4a5-73622b95b543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help at this moment, my friend.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5b787-929b-4b04-b7a2-d5a13a4fd751",
   "metadata": {},
   "source": [
    "We can do this in a more convenient way with `LangChain`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50d286-2e04-4d33-8ee3-0add3f5fe7b0",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959ffc5-7c3b-4a84-bed0-3ef28ba729c2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1dc4500-30c0-42a2-af11-6aa6fe732414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.azure_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1720017-1b60-4bac-9319-e0d485915c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000023FCA93D5D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000023FCAA8E0D0>, model_name='gpt-35-turbo-16k', temperature=0.0, openai_api_key='aac22590f900414faaf3637e450a96ae', openai_proxy='', azure_endpoint='https://a3d0dvexroai01t.openai.azure.com/', openai_api_version='2023-12-01-preview', openai_api_type='azure')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = AzureChatOpenAI(model=\"gpt-35-turbo-16k\", temperature=0, api_version=api_version)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5120784e-5d92-46e6-a92d-a6598c7c3224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.chat_models.azure_openai.AzureChatOpenAI"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b98e6-c824-45c1-8114-22cdcd598248",
   "metadata": {},
   "source": [
    "`AzureChatOpenAI` is a `LangChain` object that offers additional attributes and methods over the `AzureOpenAI` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a062b6c-0c6c-4bfd-a5f6-bf59ad74a642",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465d9b1-abe2-46c7-b264-2ea93c887bc0",
   "metadata": {},
   "source": [
    "We use the same prompt, but not as an `f-string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ad21c59-bd23-46be-91bf-680ed0338e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: ```{email}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdcd55ef-ff68-4d47-b5ff-c7f00fe65828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f1ef2-8312-4fdd-b2fe-20dd7b6f6f6a",
   "metadata": {},
   "source": [
    "The `ChatPromptTemplate` has `classmethod`s for constructing a prompt given a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ea726e9-516b-4a7f-a1cd-88f89f1c8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5c4c0d5-9a0d-4f0c-980b-39fa601c2df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['email', 'style'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['email', 'style'], template='Translate the text that is delimited by triple backticks\\ninto a style that is {style}.\\ntext: ```{email}```'))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6305928-1b22-4874-9002-1dc1d23bd02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5cf0afe5-1981-4ee1-9f93-adaf714c6655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['email', 'style'], template='Translate the text that is delimited by triple backticks\\ninto a style that is {style}.\\ntext: ```{email}```')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ca258-0def-4aba-a551-98eb2bd1a7e6",
   "metadata": {},
   "source": [
    "We can see that the `prompt_template` has identified two input variables, `email` and `style`.\n",
    "These were surrounded by braces (`{}`) in the template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b854ba0-5f33-4791-ad3e-c2e5803fb5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['email', 'style']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d14783-bc2b-42b3-8f78-1868d43edecc",
   "metadata": {},
   "source": [
    "We can set values to the input variables with `format_messages`.\n",
    "If we forget a required `input_variable`, a `KeyError` will be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d75410e2-ecac-4719-be0d-f141d30fb725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:611\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    604\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m    605\u001b[0m ):\n\u001b[0;32m    606\u001b[0m     rel_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    607\u001b[0m         k: v\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    609\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m message_template\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[0;32m    610\u001b[0m     }\n\u001b[1;32m--> 611\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrel_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    612\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:220\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseMessage]:\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m        List of BaseMessages.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:276\u001b[0m, in \u001b[0;36mHumanMessagePromptTemplate.format\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt template.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m        Formatted message.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HumanMessage(content\u001b[38;5;241m=\u001b[39mtext, additional_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs)\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain_core\\prompts\\prompt.py:132\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        prompt.format(variable1=\"foo\")\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\string.py:190\u001b[0m, in \u001b[0;36mFormatter.format\u001b[1;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Sandbox\\PRIVATE-RAI\\env\\Lib\\site-packages\\langchain_core\\utils\\formatting.py:29\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo arguments should be provided, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meverything should be passed as keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     )\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\string.py:194\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[0;32m    193\u001b[0m     used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m--> 194\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\string.py:234\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[1;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[0;32m    230\u001b[0m     auto_arg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\string.py:299\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[1;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[0;32m    297\u001b[0m     first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[1;32m--> 299\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\string.py:256\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[1;34m(self, key, args, kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'style'"
     ]
    }
   ],
   "source": [
    "prompt_template.format_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5c029de-6bf9-4477-9fc0-d0dbf6b52b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(style=style, email=email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1496431d-7c4f-47b2-88ed-80aee020429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Translate the text that is delimited by triple backticks\\ninto a style that is American English in a calm and respectful tone\\n.\\ntext: ```\\nArr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\")]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfeeb5ed-5802-4a54-8dec-b12764b71bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a246708f-d152-447b-ad1c-84e93a0b0043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.human.HumanMessage"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "429136c0-9774-46fb-8890-24c6101b70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help at this moment, my friend.\")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb645c2-eaf0-4507-bedf-5ec8af6cf601",
   "metadata": {},
   "source": [
    "We can reverse-translate our response using a similar technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2badc736-a717-4ce7-8c8f-e8979e789850",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover ]\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tought luck! See ya!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89b79106-7fae-4847-80a5-fa0f79ffe647",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_style = \"\"\"\n",
    "A polite tone \\\n",
    "that speaks in English Pirate\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bbaada62-9a15-4c9a-98ed-8f7ea7df5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_messages = prompt_template.format_messages(style=reply_style, email=reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99b2d7bc-ad0c-4ad9-990d-d6349e263f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks\n",
      "into a style that is \n",
      "A polite tone that speaks in English Pirate.\n",
      "text: ```Hey there customer, the warranty does not cover ]\n",
      "cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tought luck! See ya!```\n"
     ]
    }
   ],
   "source": [
    "print(reply_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b26d038-4015-444f-b0f5-f3d1b8a7471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr, me hearty customer, the warranty be not coverin' yer cleaning expenses fer yer galley 'cause 'tis yer fault ye misused yer blender by forgettin' to put the lid on afore startin' the blender. Tis a tough luck, matey! Fare thee well!\n"
     ]
    }
   ],
   "source": [
    "response = chat(reply_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16304b9-62cf-4503-bb9a-71a8d99eef61",
   "metadata": {},
   "source": [
    "Prompt templates provide us with a greater level of abstraction vs `f-strings` as the prompts become longer and more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84e9e3-56fd-416d-a20a-39724a1aa14d",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26792363-eab5-4ab5-922e-5ed26ef1c128",
   "metadata": {},
   "source": [
    "We can extract information with the LLM and format it as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e418ad5-826f-4f83-be75-fd7863cb2919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ae96556-da0c-47a3-b784-cc72bd88dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"\n",
    "This leaf blower is pretty amazing. It has four settings \\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversay present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "211ecc9d-9c17-464f-b583-d03f049925f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template = \"\"\"\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delviery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price, \\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ace0a43-c16f-47fc-86a3-e71a06c7d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0c6a65bc-ad29-4975-bd76-7060c3c69c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template=review_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "712b6549-9d95-4750-a604-224f50e12e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00267523-ec0d-47e7-8398-3c54dc6e5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(text=review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55704a27-d69c-4014-85fa-6b9eb4ffa1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"gift\": false,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = AzureChatOpenAI(model=\"gpt-35-turbo-16k\", temperature=0, api_version=api_version)\n",
    "response = chat(messages=messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02cc409-8f57-4adf-8284-9c429dc3e7a2",
   "metadata": {},
   "source": [
    "By default the LLM will return the response content as a string.\n",
    "We can convert it to another Python object using an `OutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6c5f9a8-d822-44fb-91a6-2832b0a811c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This raises an error because the response.content is a string.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgift\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# This raises an error because the response.content is a string.\n",
    "response.content.get(\"gift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3104a2-f577-40e6-990b-829ae038b090",
   "metadata": {},
   "source": [
    "## Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16c212ff-393b-4d33-8870-7e6a70abb352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036422aa-5434-4b4e-b99f-85ed9c861414",
   "metadata": {},
   "source": [
    "We define what we want the response to look like using the `ResponseSchema`.\n",
    "This is similar to a `pydantic.Field` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4cfebd53-6084-48ec-ac16-824de469553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased as a gift for someone else? \\\n",
    "    Answer True if yes, False if not or unknown.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc25d2f0-9752-4a5a-a678-1305ed5e590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days did it take for the product \\\n",
    "    to arrive? If this information is not found, output -1.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4583a717-2ac7-4991-b35b-9d2189ba913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any sentences about the value or price, \\\n",
    "    and output them as a comma separated Python list.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8db7540-7e8d-4430-b8a9-e0929abefafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    gift_schema,\n",
    "    delivery_days_schema,\n",
    "    price_value_schema,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1ef8c-7cf6-4a11-bcfa-efd596408102",
   "metadata": {},
   "source": [
    "We create a `StructuredOutputParser` using the `response_schemas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a95cfb48-e76d-472c-a580-6c674100bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas=response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5f5161a-f1b2-4258-a846-38e6d833128d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.output_parsers.structured.StructuredOutputParser"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9d70d-e357-4f70-ac5f-1a483cef0157",
   "metadata": {},
   "source": [
    "The response schema is formatted with instructions for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a537f47-8579-45a4-b99f-95cf2ab15e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25d3c0ff-3441-4f17-8f98-5a7a49c41263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased as a gift for someone else?     Answer True if yes, False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days did it take for the product     to arrive? If this information is not found, output -1.\n",
      "\t\"price_value\": string  // Extract any sentences about the value or price,     and output them as a comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9788b685-4e61-4461-bbf2-c053f1c3bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9e858f4-c623-4a38-a3d9-ee1071a98caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f1bccf49-e379-47c7-b79d-a402fe8c327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.format_messages(text=review, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fb898e18-dacd-4929-8e26-449c6eedddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: \n",
      "This leaf blower is pretty amazing. It has four settings candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversay present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased as a gift for someone else?     Answer True if yes, False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days did it take for the product     to arrive? If this information is not found, output -1.\n",
      "\t\"price_value\": string  // Extract any sentences about the value or price,     and output them as a comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de1475df-146f-49f2-8e0f-17019f7b9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b99da753-c188-4210-b7cd-ca371a63be86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": false,\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b74121-446a-4476-a18c-d1875556cc4d",
   "metadata": {},
   "source": [
    "The LLM does a much better job at formatting the response, though it's still a string.\n",
    "The `output_parser` can `parse` the `response.content` to the type referenced by the backticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5c5beae6-abbd-4752-a086-536af2514ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(text=response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "de2dbd41-40a3-4b96-b3e8-b28cb8aae435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False,\n",
       " 'delivery_days': '2',\n",
       " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "99ad7fa3-78a0-47d1-8d9f-650a420eaccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bdc49e35-e4f8-47fc-8801-484cbc7be433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get(\"delivery_days\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
