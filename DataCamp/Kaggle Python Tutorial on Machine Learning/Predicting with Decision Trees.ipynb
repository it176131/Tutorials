{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Intro to decision trees](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/predicting-with-decision-trees?ex=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the previous chapter, you did all the slicing and dicing yourself to find subsets that have a higher chance of surviving. A decision tree automates this process for you and outputs a classification model or classifier.\n",
    "\n",
    "Conceptually, the decision tree algorithm start with all the data at the root node and scans all the variables for the best one to split on. Once a variable is chosen, you do the split and go down one level (or one node) and repeat. The final nodes at the bottom of the decision tree are known as terminal node, and the majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node.\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Import the `numpy` library as `np`\n",
    "* From `sklearn` import the `tree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the Numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "OK, your package is loaded now. Time for the real deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Cleaning and Formatting your Data](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/predicting-with-decision-trees?ex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before you can begin constructing your trees you need to get your hands dirty and clean the data so that you can use all the features available to you. In the first chapter, we saw that the Age variable had some missing value. Missingness is a whole subject with and in itself, but we will use a simple imputation technique where we substitute each missing value with the median of the all present values.\n",
    "> `train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())`\n",
    "\n",
    "Another problem is that the Sex and Embarked variables are categorical but in a non-numeric format. Thus, we will need to assign each class a unique integer so that Python can handle the information. Embarked also has some missing values which you should impute with the most common class of embarkation, which is \"S\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Assign the integer 1 to all females\n",
    "* Impute missing values in `Embarked` with class `S`. Use `fillna()` method.\n",
    "* Replace each class of Embarked with a unique integer. `0` for `S`, `1` for `C` and `2` for `Q`.\n",
    "* Print the `Sex` and `Embarked` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      0\n",
      "5      0\n",
      "6      0\n",
      "7      0\n",
      "8      1\n",
      "9      1\n",
      "10     1\n",
      "11     1\n",
      "12     0\n",
      "13     0\n",
      "14     1\n",
      "15     1\n",
      "16     0\n",
      "17     0\n",
      "18     1\n",
      "19     1\n",
      "20     0\n",
      "21     0\n",
      "22     1\n",
      "23     0\n",
      "24     1\n",
      "25     1\n",
      "26     0\n",
      "27     0\n",
      "28     1\n",
      "29     0\n",
      "      ..\n",
      "861    0\n",
      "862    1\n",
      "863    1\n",
      "864    0\n",
      "865    1\n",
      "866    1\n",
      "867    0\n",
      "868    0\n",
      "869    0\n",
      "870    0\n",
      "871    1\n",
      "872    0\n",
      "873    0\n",
      "874    1\n",
      "875    1\n",
      "876    0\n",
      "877    0\n",
      "878    0\n",
      "879    1\n",
      "880    1\n",
      "881    0\n",
      "882    1\n",
      "883    0\n",
      "884    0\n",
      "885    1\n",
      "886    0\n",
      "887    1\n",
      "888    1\n",
      "889    0\n",
      "890    0\n",
      "Name: Sex, Length: 891, dtype: int64\n",
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "5      2\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "9      1\n",
      "10     0\n",
      "11     0\n",
      "12     0\n",
      "13     0\n",
      "14     0\n",
      "15     0\n",
      "16     2\n",
      "17     0\n",
      "18     0\n",
      "19     1\n",
      "20     0\n",
      "21     0\n",
      "22     2\n",
      "23     0\n",
      "24     0\n",
      "25     0\n",
      "26     1\n",
      "27     0\n",
      "28     2\n",
      "29     0\n",
      "      ..\n",
      "861    0\n",
      "862    0\n",
      "863    0\n",
      "864    0\n",
      "865    0\n",
      "866    1\n",
      "867    0\n",
      "868    0\n",
      "869    0\n",
      "870    0\n",
      "871    0\n",
      "872    0\n",
      "873    0\n",
      "874    1\n",
      "875    1\n",
      "876    0\n",
      "877    0\n",
      "878    0\n",
      "879    1\n",
      "880    0\n",
      "881    0\n",
      "882    0\n",
      "883    0\n",
      "884    0\n",
      "885    2\n",
      "886    0\n",
      "887    0\n",
      "888    0\n",
      "889    1\n",
      "890    2\n",
      "Name: Embarked, Length: 891, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the train and test datasets to create two DataFrames\n",
    "train_url = 'http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv'\n",
    "train = pd.read_csv(train_url)\n",
    "\n",
    "test_url = 'http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv'\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "train.Age = train.Age.fillna(train.Age.median())\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "train.loc[train.Sex == 'male', 'Sex'] = 0\n",
    "train.loc[train.Sex == 'female', 'Sex'] = 1\n",
    "\n",
    "# Impute the Embarked variable\n",
    "train.Embarked = train.Embarked.fillna('S')\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "train.loc[train.Embarked == 'S', 'Embarked'] = 0\n",
    "train.loc[train.Embarked == 'C', 'Embarked'] = 1\n",
    "train.loc[train.Embarked == 'Q', 'Embarked'] = 2\n",
    "\n",
    "# Print the Sex and Embarked columns\n",
    "print(train.Sex)\n",
    "print(train.Embarked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great! Now that the data is cleaned up a bit you are ready to begin building your first decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Creating your firs decision tree](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/predicting-with-decision-trees?ex=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You will use the `scikit-learn` and `numpy` libraries to build your first decision tree. `scikit-learn` can be used to create `tree` objects from the `DecisionTreeClassifier` class. The methods that we will use take `numpy` arrays as input and therefore we will need to create those from the `DataFrame` that we already have. We will need the following to build a decision tree.\n",
    "* `target`: A one-dimensional numpy array containing the target/response from the train data. (Survival in your case)\n",
    "* `features`: A multidimensional numpy array containing the features/predictors from the train data. (e.g. Sex, Age)\n",
    "\n",
    "Take a look at the sample code below to see what this would look like:\n",
    "> `target = train[\"Survived\"].values`\n",
    "\n",
    "> `features = train[[\"Sex\", \"Age\"]].values`\n",
    "\n",
    "> `my_tree = tree.DecisionTreeClassifier()`\n",
    "\n",
    "> `my_tree = my_tree.fit(features, target)`\n",
    "\n",
    "One way to quickly see the result of your decision tree is to see the importance of the features that are included. This is done by requesting the `.feature_importances_` attribute of your tree object. Another quick metric is the mean accuracy that you can compute using the `.score()` function with `features_one` and `target` as arguments.\n",
    "\n",
    "Okay, time for you to build your first decision tree in Python! The train and testing data from chapter 1 are available in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Build the `target` and `features_one` numpy arrays. The target will be based on the `Survived` colum in `train`. The features array will be based on the variables Passenger, Class, Sex, Age, and Passenger Fare\n",
    "* Build a decision tree `my_tree_one` to predict survival using `features_one` and `target`\n",
    "* Look at the importance of features in your tree and compute the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "5              6         0       3   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "8              9         1       3   \n",
      "9             10         1       2   \n",
      "10            11         1       3   \n",
      "11            12         1       1   \n",
      "12            13         0       3   \n",
      "13            14         0       3   \n",
      "14            15         0       3   \n",
      "15            16         1       2   \n",
      "16            17         0       3   \n",
      "17            18         1       2   \n",
      "18            19         0       3   \n",
      "19            20         1       3   \n",
      "20            21         0       2   \n",
      "21            22         1       2   \n",
      "22            23         1       3   \n",
      "23            24         1       1   \n",
      "24            25         0       3   \n",
      "25            26         1       3   \n",
      "26            27         0       3   \n",
      "27            28         0       1   \n",
      "28            29         1       3   \n",
      "29            30         0       3   \n",
      "..           ...       ...     ...   \n",
      "861          862         0       2   \n",
      "862          863         1       1   \n",
      "863          864         0       3   \n",
      "864          865         0       2   \n",
      "865          866         1       2   \n",
      "866          867         1       2   \n",
      "867          868         0       1   \n",
      "868          869         0       3   \n",
      "869          870         1       3   \n",
      "870          871         0       3   \n",
      "871          872         1       1   \n",
      "872          873         0       1   \n",
      "873          874         0       3   \n",
      "874          875         1       2   \n",
      "875          876         1       3   \n",
      "876          877         0       3   \n",
      "877          878         0       3   \n",
      "878          879         0       3   \n",
      "879          880         1       1   \n",
      "880          881         1       2   \n",
      "881          882         0       3   \n",
      "882          883         0       3   \n",
      "883          884         0       2   \n",
      "884          885         0       3   \n",
      "885          886         0       3   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name  Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    0  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina    1  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1   \n",
      "4                             Allen, Mr. William Henry    0  35.0      0   \n",
      "5                                     Moran, Mr. James    0  28.0      0   \n",
      "6                              McCarthy, Mr. Timothy J    0  54.0      0   \n",
      "7                       Palsson, Master. Gosta Leonard    0   2.0      3   \n",
      "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    1  27.0      0   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)    1  14.0      1   \n",
      "10                     Sandstrom, Miss. Marguerite Rut    1   4.0      1   \n",
      "11                            Bonnell, Miss. Elizabeth    1  58.0      0   \n",
      "12                      Saundercock, Mr. William Henry    0  20.0      0   \n",
      "13                         Andersson, Mr. Anders Johan    0  39.0      1   \n",
      "14                Vestrom, Miss. Hulda Amanda Adolfina    1  14.0      0   \n",
      "15                    Hewlett, Mrs. (Mary D Kingcome)     1  55.0      0   \n",
      "16                                Rice, Master. Eugene    0   2.0      4   \n",
      "17                        Williams, Mr. Charles Eugene    0  28.0      0   \n",
      "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...    1  31.0      1   \n",
      "19                             Masselmani, Mrs. Fatima    1  28.0      0   \n",
      "20                                Fynney, Mr. Joseph J    0  35.0      0   \n",
      "21                               Beesley, Mr. Lawrence    0  34.0      0   \n",
      "22                         McGowan, Miss. Anna \"Annie\"    1  15.0      0   \n",
      "23                        Sloper, Mr. William Thompson    0  28.0      0   \n",
      "24                       Palsson, Miss. Torborg Danira    1   8.0      3   \n",
      "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...    1  38.0      1   \n",
      "26                             Emir, Mr. Farred Chehab    0  28.0      0   \n",
      "27                      Fortune, Mr. Charles Alexander    0  19.0      3   \n",
      "28                       O'Dwyer, Miss. Ellen \"Nellie\"    1  28.0      0   \n",
      "29                                 Todoroff, Mr. Lalio    0  28.0      0   \n",
      "..                                                 ...  ...   ...    ...   \n",
      "861                        Giles, Mr. Frederick Edward    0  21.0      1   \n",
      "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...    1  48.0      0   \n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"    1  28.0      8   \n",
      "864                             Gill, Mr. John William    0  24.0      0   \n",
      "865                           Bystrom, Mrs. (Karolina)    1  42.0      0   \n",
      "866                       Duran y More, Miss. Asuncion    1  27.0      1   \n",
      "867               Roebling, Mr. Washington Augustus II    0  31.0      0   \n",
      "868                        van Melkebeke, Mr. Philemon    0  28.0      0   \n",
      "869                    Johnson, Master. Harold Theodor    0   4.0      1   \n",
      "870                                  Balkic, Mr. Cerin    0  26.0      0   \n",
      "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)    1  47.0      1   \n",
      "872                           Carlsson, Mr. Frans Olof    0  33.0      0   \n",
      "873                        Vander Cruyssen, Mr. Victor    0  47.0      0   \n",
      "874              Abelson, Mrs. Samuel (Hannah Wizosky)    1  28.0      1   \n",
      "875                   Najib, Miss. Adele Kiamie \"Jane\"    1  15.0      0   \n",
      "876                      Gustafsson, Mr. Alfred Ossian    0  20.0      0   \n",
      "877                               Petroff, Mr. Nedelio    0  19.0      0   \n",
      "878                                 Laleff, Mr. Kristo    0  28.0      0   \n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)    1  56.0      0   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)    1  25.0      0   \n",
      "881                                 Markun, Mr. Johann    0  33.0      0   \n",
      "882                       Dahlberg, Miss. Gerda Ulrika    1  22.0      0   \n",
      "883                      Banfield, Mr. Frederick James    0  28.0      0   \n",
      "884                             Sutehall, Mr. Henry Jr    0  25.0      0   \n",
      "885               Rice, Mrs. William (Margaret Norton)    1  39.0      0   \n",
      "886                              Montvila, Rev. Juozas    0  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith    1  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"    1  28.0      1   \n",
      "889                              Behr, Mr. Karl Howell    0  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    0  32.0      0   \n",
      "\n",
      "     Parch            Ticket      Fare        Cabin  Embarked  \n",
      "0        0         A/5 21171    7.2500          NaN         0  \n",
      "1        0          PC 17599   71.2833          C85         1  \n",
      "2        0  STON/O2. 3101282    7.9250          NaN         0  \n",
      "3        0            113803   53.1000         C123         0  \n",
      "4        0            373450    8.0500          NaN         0  \n",
      "5        0            330877    8.4583          NaN         2  \n",
      "6        0             17463   51.8625          E46         0  \n",
      "7        1            349909   21.0750          NaN         0  \n",
      "8        2            347742   11.1333          NaN         0  \n",
      "9        0            237736   30.0708          NaN         1  \n",
      "10       1           PP 9549   16.7000           G6         0  \n",
      "11       0            113783   26.5500         C103         0  \n",
      "12       0         A/5. 2151    8.0500          NaN         0  \n",
      "13       5            347082   31.2750          NaN         0  \n",
      "14       0            350406    7.8542          NaN         0  \n",
      "15       0            248706   16.0000          NaN         0  \n",
      "16       1            382652   29.1250          NaN         2  \n",
      "17       0            244373   13.0000          NaN         0  \n",
      "18       0            345763   18.0000          NaN         0  \n",
      "19       0              2649    7.2250          NaN         1  \n",
      "20       0            239865   26.0000          NaN         0  \n",
      "21       0            248698   13.0000          D56         0  \n",
      "22       0            330923    8.0292          NaN         2  \n",
      "23       0            113788   35.5000           A6         0  \n",
      "24       1            349909   21.0750          NaN         0  \n",
      "25       5            347077   31.3875          NaN         0  \n",
      "26       0              2631    7.2250          NaN         1  \n",
      "27       2             19950  263.0000  C23 C25 C27         0  \n",
      "28       0            330959    7.8792          NaN         2  \n",
      "29       0            349216    7.8958          NaN         0  \n",
      "..     ...               ...       ...          ...       ...  \n",
      "861      0             28134   11.5000          NaN         0  \n",
      "862      0             17466   25.9292          D17         0  \n",
      "863      2          CA. 2343   69.5500          NaN         0  \n",
      "864      0            233866   13.0000          NaN         0  \n",
      "865      0            236852   13.0000          NaN         0  \n",
      "866      0     SC/PARIS 2149   13.8583          NaN         1  \n",
      "867      0          PC 17590   50.4958          A24         0  \n",
      "868      0            345777    9.5000          NaN         0  \n",
      "869      1            347742   11.1333          NaN         0  \n",
      "870      0            349248    7.8958          NaN         0  \n",
      "871      1             11751   52.5542          D35         0  \n",
      "872      0               695    5.0000  B51 B53 B55         0  \n",
      "873      0            345765    9.0000          NaN         0  \n",
      "874      0         P/PP 3381   24.0000          NaN         1  \n",
      "875      0              2667    7.2250          NaN         1  \n",
      "876      0              7534    9.8458          NaN         0  \n",
      "877      0            349212    7.8958          NaN         0  \n",
      "878      0            349217    7.8958          NaN         0  \n",
      "879      1             11767   83.1583          C50         1  \n",
      "880      1            230433   26.0000          NaN         0  \n",
      "881      0            349257    7.8958          NaN         0  \n",
      "882      0              7552   10.5167          NaN         0  \n",
      "883      0  C.A./SOTON 34068   10.5000          NaN         0  \n",
      "884      0   SOTON/OQ 392076    7.0500          NaN         0  \n",
      "885      5            382652   29.1250          NaN         2  \n",
      "886      0            211536   13.0000          NaN         0  \n",
      "887      0            112053   30.0000          B42         0  \n",
      "888      2        W./C. 6607   23.4500          NaN         0  \n",
      "889      0            111369   30.0000         C148         1  \n",
      "890      0            370376    7.7500          NaN         2  \n",
      "\n",
      "[891 rows x 12 columns]\n",
      "[ 0.12315342  0.31274009  0.24106694  0.32303954]\n",
      "0.977553310887\n"
     ]
    }
   ],
   "source": [
    "# Print the train data to see the available features\n",
    "print(train)\n",
    "\n",
    "# Create the target and features numpy arrays: target, features_one\n",
    "target = train.Survived.values\n",
    "features_one = train[['Pclass', 'Sex', 'Age', 'Fare']].values\n",
    "\n",
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "my_tree_one = my_tree_one.fit(features_one, target)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(features_one, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Well done! Time to investigate your decision tree a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Interpreting your decision tree](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/predicting-with-decision-trees?ex=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `feature_importances_` attribute make it simple to interpret the significance of the predictors you include. Based on your decision tree, what variable plays the most important role in determing whether or not a passenger survived? Your model (`my_tree_one`) is available in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12315342,  0.31274009,  0.24106694,  0.32303954])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tree_one.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bellissimo! Time to make a prediction and submit it to Kaggle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Predict and submit to Kaggle](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/predicting-with-decision-trees?ex=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To send a submission to Kaggle you need to predict the survival rates for the observations in the test set. In the last exercise of the previous chapter, we created simple predictions based on a single subset. Luckily, with our decision tree, we can make use of some simple functions to \"generate\" our answer without having to manually perform subsetting.\n",
    "\n",
    "First, you make use of the `.predict()` method. You provide it the model (`my_tree_one`), the values of features from the dataset for which predictions need to be made (`test`). To extract the features we will need to create a numpy array in the same way as we did when training the model. However, we need to take care of a small but important problem first. There is a missing value in the Fare feature that needs to be imputed.\n",
    "\n",
    "Next, you need to make sure your output is in line with the submission requirements of Kaggle: a csv file with exactly 418 entries and two columns: `PassengerId` and `Survived`. Then use tyhe code provided to make a new data frame using `DataFrame()`, and create a csv file using `to_csv()` method from Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Impute the missing value for Fare in row 153 with the median of the column.\n",
    "* Make a prediction on the test set using the `.predict()` method and `my_tree_one`. Assign the result to `my_prediction`.\n",
    "* Create a data frame `my_solution` containing the solution and the passenger ids from the test set. Make sure the solution is in line with the standards set forth by Kaggle by naming the column appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0\n",
      " 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0\n",
      " 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
      " 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0\n",
      " 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0\n",
      " 0 1 1 1 1 0 0 1 0 0 0]\n",
      "      Survived\n",
      "892          0\n",
      "893          0\n",
      "894          1\n",
      "895          1\n",
      "896          1\n",
      "897          0\n",
      "898          0\n",
      "899          0\n",
      "900          1\n",
      "901          0\n",
      "902          0\n",
      "903          0\n",
      "904          1\n",
      "905          1\n",
      "906          1\n",
      "907          1\n",
      "908          0\n",
      "909          1\n",
      "910          1\n",
      "911          0\n",
      "912          0\n",
      "913          1\n",
      "914          1\n",
      "915          0\n",
      "916          1\n",
      "917          0\n",
      "918          1\n",
      "919          1\n",
      "920          1\n",
      "921          0\n",
      "...        ...\n",
      "1280         0\n",
      "1281         0\n",
      "1282         1\n",
      "1283         1\n",
      "1284         0\n",
      "1285         0\n",
      "1286         0\n",
      "1287         1\n",
      "1288         0\n",
      "1289         1\n",
      "1290         0\n",
      "1291         0\n",
      "1292         1\n",
      "1293         0\n",
      "1294         1\n",
      "1295         1\n",
      "1296         0\n",
      "1297         0\n",
      "1298         0\n",
      "1299         0\n",
      "1300         1\n",
      "1301         1\n",
      "1302         1\n",
      "1303         1\n",
      "1304         0\n",
      "1305         0\n",
      "1306         1\n",
      "1307         0\n",
      "1308         0\n",
      "1309         0\n",
      "\n",
      "[418 rows x 1 columns]\n",
      "(418, 1)\n"
     ]
    }
   ],
   "source": [
    "test.loc[test.Sex == 'male', 'Sex'] = 0\n",
    "test.loc[test.Sex == 'female', 'Sex'] = 1\n",
    "\n",
    "test.Age = test.Age.fillna(test.Age.median())\n",
    "\n",
    "# Impute the missing value with the median\n",
    "test.loc[152, 'Fare'] = test.Fare.median()\n",
    "\n",
    "# Extract the features from the test set: Pclass, Sex, Age, and Fare.\n",
    "test_features = test[['Pclass', 'Sex', 'Age', 'Fare']].values\n",
    "\n",
    "# Make your prediction using the test set\n",
    "my_prediction = my_tree_one.predict(test_features)\n",
    "print(my_prediction)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\n",
    "PassengerId = np.array(test.PassengerId).astype(int)\n",
    "my_solution = pd.DataFrame(my_prediction, PassengerId, columns = ['Survived'])\n",
    "print(my_solution)\n",
    "\n",
    "# Check that your data frame has 418 entries\n",
    "print(my_solution.shape)\n",
    "\n",
    "# Write your solution to a csv file with the name my_solution.csv\n",
    "my_solution.to_csv('my_solution_one.csv', index_label = ['PassengerId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great! You just created your first decision tree. [Download your csv file](https://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/my_solution_one.csv), and submit the created csv to Kaggle to see the result of your effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Overfitting and how to control it](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/predicting-with-decision-trees?ex=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When you created your first decision tree the default arguments for `max_depth` and `min_sample_split` were set to `None`. This means that no limit on the depth of your tree was set. That's a good thing right? Not so fast. We are likely overfitting. This means that while your model describes the training data extremely well, it doesn't generalize to new data, which is frankly the point of prediction. Just look at the Kaggle submissions results for the simple model based on Gender and the complex decision tree. Which one does better?\n",
    "\n",
    "Maybe we can improve the overfit model by making a less complex model? In `DecisionTreeRegressor`, the depth of our model is defined by two paramters:\n",
    "* the `max_depth` parameter determines when the splitting up of the decision tree stops.\n",
    "* the `min_samples_split` parameter monitors the amount of observations in a bucket. If a certain threshold is not reached (e.g. minimu 10 passengers) no further splitting can be done.\n",
    "\n",
    "By limiting the complexity of your decision tree you will increase its generality and thus its usefulness for prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Include the Siblings/Spouse Aboard, Parents/Children Aboard, and Embarked features in a new set of features.\n",
    "* Fit your second tree `my_tree_two` with the new features, and control for the model complexity by toggling the `max_depth` and `min_samples_split` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905723905724\n"
     ]
    }
   ],
   "source": [
    "# Create a new array with the added features: features_two\n",
    "features_two = train[['Pclass', 'Age', 'Sex', 'Fare', 'SibSp', 'Parch', 'Embarked']].values\n",
    "\n",
    "# Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5: my_tree_two\n",
    "max_depth = 10\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split, random_state = 1)\n",
    "my_tree_two = my_tree_two.fit(features_two, target)\n",
    "\n",
    "# Print the score of the new decision tree\n",
    "print(my_tree_two.score(features_two, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great! You just created your second and possibly improved decision tree. [Download your csv file](https://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/my_solution_two.csv). Submit your updated solution to Kaggle to see how despite a lower `.score` you predict better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [Feature-engineering for our Titanic data set](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning/predicting-with-decision-trees?ex=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data Science is an art that benefits from a human element. Enter feature engineering: creatively engineering your own features by combining the different existing variables.\n",
    "\n",
    "While feature engineering is a discipline in itself, too broad to be covered here in detail, you will have a look at a simple example by creating your own new predictive attribute: `family_size`.\n",
    "\n",
    "A valid assumption is that larger families need more time to get together on a sinking ship, and hence have lower probability of surviving. Family size is determined by the variables `SibSp` and `Parch`, which indicate the number of family members a certain passenger is traveling with. So when doing feature engineering, you add a new variable `family_size`, which is the sum of `SibSp` and `Parch` plus one (the observation itself), to the test and train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Create a new train set `train_two` that differs from `train` only by having an extra column with your feature engineered variable `family_size`.\n",
    "* Add your feature engineered variable `family_size` in addition to `Pclass`, `Sex`, `Age`, `Fare`, `SibSp`, and `Parch` to `features_three`.\n",
    "* Create a new decision tree as `my_tree_three` and fit the decision tree with your new feature set `features_three`. Then check out the the score of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979797979798\n"
     ]
    }
   ],
   "source": [
    "# Create train_two with the newly defined feature\n",
    "train_two = train.copy()\n",
    "train_two[\"family_size\"] = train_two.SibSp + train_two.Parch + 1\n",
    "\n",
    "# Create a new feature set and add the new feature\n",
    "features_three = train_two[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", \"family_size\"]].values\n",
    "\n",
    "# Define the tree classifier, then fit the model\n",
    "my_tree_three = tree.DecisionTreeClassifier()\n",
    "my_tree_three = my_tree_three.fit(features_three, target)\n",
    "\n",
    "# Print the score of this decision tree\n",
    "print(my_tree_three.score(features_three, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great! Notice that this time the newly created variable is included in the model. [Download your csv file](https://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/my_solution_three.csv), and submit the created csv to Kaggle to see the result of the updated model.\n",
    "\n",
    "You have finished the chapter \"Predicting with Decision Trees\"!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
