{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2.1 What Is Statistical Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to motivate our study of statistical learning, we begin with a simple example. Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. The `Advertising` data set consists of the `sales` of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: `TV`, `radio`, and `newspaper`. The data are displayed in Figure 2.1. It is not possible for our client to directly increase sales of the product. On the other hand, they can control the advertising expenditure in each of the three media. Therefore, if we determine that there is an association between advertising and sales, then we can instruct our client to adjust advertising budgets, thereby indirectly increasing sales. In other words, our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.\n",
    "\n",
    "In this setting the advertising budgets are $input ~variables$ while `sales` is an $output ~variable$. The input variables are typically denoted using the symbol $X$, with a subscript to distinguish them. So $X_1$ might be the `TV` budgets, $X_2$ the `radio` budget, and $X_3$ the `newspaper` budget. The inputs go by different names, such as $predictors$, $independent ~variables$, $features$, or sometimes just $variables$. The output variable -- in this case, `sales` -- is often called the $response$ or $dependent ~variable$, and is typically denoted using the symbol $Y$. Throughout this book, we will use all of these terms interchangeably.\n",
    "\n",
    "More generally, suppose that we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \\dots, X_p$. We assume that there is some relationship between $Y$ and $X = (X_1, X_2, \\dots, X_p)$, which can be written in the very general form\n",
    "\n",
    "$$Y = f(X) + \\epsilon \\tag{2.1}$$\n",
    "\n",
    "Here $f$ is some fixed but unknown function of $X_1, \\dots, X_p$, and $\\epsilon$ is a random $error ~term$, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the $systematic$ information that $X$ provides about $Y$.\n",
    "\n",
    "As another example, consider the left-hand panel of Figure 2.2, a plot of `income` versus `years of education` for 30 individuals in the `Income` data set. The plot suggests that one might be able to predict `income` using `years of education`. However, the function $f$ that connects the input variable to the output variable is in general unknown. In this situation one must estimate $f$ based on the observed points. Since `Income` is a simulated data set, $f$ is known and is shown by the blue curve in the right-hand panel of Figure 2.2. The vertical lines represent the error terms $\\epsilon$. We note that some of the 30 observations lie above the blue curve and some lie below it; overall, the errors have approximately mean zero.\n",
    "\n",
    "In general, the function $f$ may involve more than one input variable. In Figure 2.3 we plot `income` as a function of `years of education` and `seniority`. Here $f$ is a two-dimensional surface that must be estimated based on the observed data.\n",
    "\n",
    "In essence, statistical learning refers to a set of approaches for estimating $f$. In this chapter we outline some of the key theoretical concepts that arise in estimating $f$, as well as tools for evaluating the estimates obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## $2.1.1 ~Why ~Estimate ~f?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In many situations, a set of inputs $X$ are readily available, but the output $Y$ cannot be easily obtained. In this setting, since the error term averages to zero, we can predict $Y$ using\n",
    "\n",
    "$$\\hat{Y} = \\hat{f}(X), \\tag{2.2}$$\n",
    "\n",
    "where $\\hat{f}$ represents our estimate for $f$, and $\\hat{Y}$ represents the resulting prediction for $Y$. In this setting, $\\hat{f}$ is often treated as a $black ~box$, in the sense that one is not typically concerned with the exact form of $\\hat{f}$, provided that it yields accurate predictions for $Y$.\n",
    "\n",
    "As an example, suppose that $X_1, \\dots, X_p$ are characteristics of a patient's blood sample that can be easily measured in a lab, and $Y$ is a variable encoding the patient's risk for a sever adverse reaction to a particular drug. It is natural to seek to predict $Y$ using $X$, since we can then avoid giving the drug in question to patient's who are at high risk of an adverse reaction -- that is, patient's for whom the estimate of $Y$ is high.\n",
    "\n",
    "The accuracy of $\\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we will call the $reducible ~error$ and the $irreducible ~error$. In general, $\\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. This error is $reducible$ because we can potentially improve the accuracy of $\\hat{f}$ by using the most appropriate statistical learning technique to estimate $f$. However, even if it were possible to form a perfect estimate for $f$, so that our estimated response took the form $\\hat{Y} = f(X)$, our prediction would still have some error in it! This is because $Y$ is also a function of $\\epsilon$, which, by definition, cannot be predicted using $X$. Therefore, variability associated with $\\epsilon$ also affects the accuracy of our predictions. This is known as the $irreducible$ error, because no matter how well we estimate $f$, we cannot reduce the error introduced by $\\epsilon$.\n",
    "\n",
    "Why is the irreducible error larger than zero? The quantity $\\epsilon$ may contain unmeasured variables that are useful in predicting $Y$: since we don't measure them, $f$ cannot use them for its prediction. The quantity $\\epsilon$ may also contain unmeasurable variation. For example, the risk of an adverse reaction might vary for a given patient on a given day, depending on manufacturing variation in the drug itself or the patient's general feeling of well-being on that day.\n",
    "\n",
    "Consider a given estimate $\\hat{f}$ and a set of predictors $X$, which yields the prediction $\\hat{Y} = \\hat{f}(X)$. Assume for a moment that both $\\hat{f}$ and $X$ are fixed. Then, it is easy to show that\n",
    "\n",
    "$$\\eqalignno{\n",
    "E(Y - \\hat{Y})^2 & = E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\\n",
    "                 & = \\underbrace{[f(X) - \\hat{f}(X)]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}, \\tag{2.3}\n",
    "}$$\n",
    "\n",
    "where $E(Y - \\hat{Y})^2$ represents the average, or $expected ~value$, of the squared difference between the predicted and actual value of $Y$, and $Var(\\epsilon)$ represents the $variance$ associated with the error term $\\epsilon$.\n",
    "\n",
    "The focus of this book is on techniques for estimating $f$ with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$. This bound is almost always unknown in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
